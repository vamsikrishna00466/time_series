{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "for tomorrow.ipynb",
      "provenance": [],
      "mount_file_id": "1SQdqqoY-gNMhMmOCWuMAWnJ9JMkSDdIR",
      "authorship_tag": "ABX9TyMva0MuwGLTIWlaUB+flJND",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vamsikrishna00466/time_series/blob/main/Multi%20variate%20time%20series%20by%20VAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jrCq0Ex1WP0"
      },
      "outputs": [],
      "source": [
        "#importing pandas and warnings\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Importing data\n",
        "df=pd.read_excel('/content/drive/MyDrive/SCA Projects/Forecasting /Raw Data for Forecasting Project.xlsx')\n",
        "\n",
        "##Initial preprocessing(12-12-2021) work-Function\n",
        "def data_preprocess1(df):\n",
        "  df = df.drop(df.columns.to_series()[\"Unnamed: 6\":\"Receipt Details\"], axis=1)\n",
        "  df.columns = df.iloc[0]\n",
        "  df = df[1:].reset_index(drop=bool)\n",
        "  del df['Total']\n",
        "  df = df.melt(id_vars=[\"Part No.\", \"Unit Price\",'Category','Sales MOQ','Opening Stock'],\n",
        "               var_name=\"Month\",\n",
        "               value_name=\"demand\")\n",
        "  df['Month']=pd.to_datetime(df['Month']).dt.to_period('M')\n",
        "  df['Unit Price']=df['Unit Price'].astype('float')\n",
        "  df['Sales MOQ']=df['Sales MOQ'].astype('float')\n",
        "  df['Opening Stock']=df['Opening Stock'].astype('float')\n",
        "  df['demand']=df['demand'].astype('float')\n",
        "  return df\n",
        "\n",
        "##Initial preprocessing raw data\n",
        "df1 = data_preprocess1(df)\n",
        "\n",
        "##Basic Analysis Function for initial preprocessed data\n",
        "def basic_analysis(df):\n",
        "  print('*****Initial Information about Data*****\\n')\n",
        "  print(df.head())\n",
        "  print('\\nNumber of Rows and Columns:',df.shape,'\\n--------------------')\n",
        "  Null = df.isnull().sum()\n",
        "  Null = Null[Null>0]\n",
        "  print('missing data:\\n',Null)\n",
        "  print('--------------------\\nData Types:\\n',df.dtypes)\n",
        "  print('--------------------\\nCategorical variables:',df.select_dtypes(include=['object']).columns.to_list(),'And Number of columns:',len(df.select_dtypes(include=['object']).columns.to_list()))\n",
        "  print('--------------------\\nNumerical variables:',df.select_dtypes(include=[int,float]).columns.to_list(),'And Number of columns:',len(df.select_dtypes(include=[int,float]).columns.to_list()))\n",
        "  print('--------------------\\nDate time variables:',df.select_dtypes(include=['period[M]']).columns.to_list())\n",
        "  print('--------------------\\n')\n",
        "  print(round(df.describe()).astype(int))\n",
        "  print('--------------------\\n')\n",
        "  print(df.describe(include=object))\n",
        "  print('--------------------\\n')\n",
        "  print(df['Month'].describe())\n",
        "  print('--------------------\\n')\n",
        "\n",
        "#Basic information about initial pre processed data\n",
        "basic_analysis(df1)\n",
        "\n",
        "#group by month and categorya and aggregation with demand sum\n",
        "df2 = df1.groupby(['Category','Month'])['demand'].sum().reset_index()\n",
        "print('Group by ccompleted')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.catplot(x=\"Month\", y=\"demand\", hue='Category', data=df2, kind='point',height=5)\n",
        "plt.show()\n",
        "\n",
        "##converting categories into columns\n",
        "df3 = df2.pivot(index='Month', columns='Category', values='demand')\n",
        "\n",
        "print('Rows and columns of categories columns month wise demand:',df3.shape)\n",
        "\n",
        "##sort data by date(ascending order)\n",
        "df3.sort_index(inplace=True)\n",
        "\n",
        "df3 = round(df3).astype(int)\n",
        "\n",
        "print(round(df3.describe()).astype(int))\n",
        "\n",
        "## Hyphothesis Testing for distribution checking(normally distributed or not)\n",
        "alpha = 0.5\n",
        "H0 = 'Data is normal'\n",
        "Ha = 'Data is not normal'\n",
        "from scipy.stats import shapiro\n",
        "for i in df3.columns.to_list():\n",
        "  p = round(shapiro(df3[i])[1], 2)\n",
        "  print(f'**************{i}**************')\n",
        "  if p > alpha:\n",
        "    print(f\"{p} > {alpha}. We fail to reject Null Hypothesis. {H0}\")\n",
        "  else:\n",
        "    print(f\"{p} <= {alpha}. We reject Null Hypothesis. {Ha}\")\n",
        "  print('--------------\\n')\n",
        "\n",
        "##ploting line chart of categoris with respect to demand\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "for i in df3.columns.to_list():\n",
        "  df3[i].plot(label=i)\n",
        "  plt.suptitle(i)\n",
        "  plt.show()\n",
        "  print('\\n')\n",
        "\n",
        "##grangercausalitytests test\n",
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "maxlag=12\n",
        "test = 'ssr_chi2test'\n",
        "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
        "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
        "    The rows are the response variable, columns are predictors. The values in the table \n",
        "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
        "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
        "    zero, that is, the X does not cause Y can be rejected.\n",
        "\n",
        "    data      : pandas dataframe containing the time series variables\n",
        "    variables : list containing names of the time series variables.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
        "    for c in df.columns:\n",
        "        for r in df.index:\n",
        "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
        "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
        "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
        "            min_p_value = np.min(p_values)\n",
        "            df.loc[r, c] = min_p_value\n",
        "    df.columns = [var + '_x' for var in variables]\n",
        "    df.index = [var + '_y' for var in variables]\n",
        "    return df\n",
        "\n",
        "grangers_causation_matrix(df3, variables = df3.columns)        \n",
        "\n",
        "\n",
        "##coint_johansen test\n",
        "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
        "def cointegration_test(df, alpha=0.05): \n",
        "    \"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\n",
        "    out = coint_johansen(df,-1,5)\n",
        "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
        "    traces = out.lr1\n",
        "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
        "    def adjust(val, length= 6): return str(val).ljust(length)\n",
        "\n",
        "    # Summary\n",
        "    print('Name   ::  Test Stat > C(95%)    =>   Signif  \\n', '--'*20)\n",
        "    for col, trace, cvt in zip(df.columns, traces, cvts):\n",
        "        print(adjust(col), ':: ', adjust(round(trace,2), 9), \">\", adjust(cvt, 8), ' =>  ' , trace > cvt)\n",
        "\n",
        "cointegration_test(df3)\n",
        "\n",
        "##Stationary test\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "def adfuller_test(series, signif=0.05, name='', verbose=False):\n",
        "    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n",
        "    r = adfuller(series, autolag='AIC')\n",
        "    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n",
        "    p_value = output['pvalue'] \n",
        "    def adjust(val, length= 6): return str(val).ljust(length)\n",
        "\n",
        "    # Print Summary\n",
        "    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n",
        "    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n",
        "    print(f' Significance Level    = {signif}')\n",
        "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
        "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
        "\n",
        "    for key,val in r[4].items():\n",
        "        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n",
        "\n",
        "    if p_value <= signif:\n",
        "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
        "        print(f\" => Series is Stationary.\")\n",
        "    else:\n",
        "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
        "        print(f\" => Series is Non-Stationary.\") \n",
        "\n",
        "# ADF Test on each column\n",
        "for name, column in df3.iteritems():\n",
        "    adfuller_test(column, name=column.name)\n",
        "    print('\\n')\n",
        "\n",
        "# 1st difference for make data stationary\n",
        "df3_differenced = df3.diff().dropna()\n",
        "\n",
        "# ADF Test on each column again\n",
        "for name, column in df3_differenced.iteritems():\n",
        "    adfuller_test(column, name=column.name)\n",
        "    print('\\n')\n",
        "\n",
        "#creating the train and validation set\n",
        "train = df3[:int(0.8*(len(df3)))]\n",
        "valid = df3[int(0.8*(len(df3))):]\n",
        "\n",
        "#fit the model\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "\n",
        "model = VAR(endog=train)\n",
        "model_fit = model.fit()\n",
        "\n",
        "# make prediction on validation\n",
        "prediction = model_fit.forecast(model_fit.y, steps=len(valid))\n",
        "\n",
        "#mean squred error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print('Mean squre Error:',round(np.sqrt(mean_squared_error(valid, prediction, squared=False))))\n"
      ]
    }
  ]
}