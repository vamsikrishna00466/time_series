{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOnbYNTBoP5TuxIXPhLWt6O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vamsikrishna00466/time_series/blob/main/Multivariate%20Time%20Series%20Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing Necessary Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "##Hyphothesis Testing Libraries \n",
        "from scipy.stats import shapiro\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
        "\n",
        "## Algorithms for forcasting\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "\n",
        "##Matrix for undestanding the permance \n",
        "from sklearn.metrics import mean_absolute_percentage_error\n"
      ],
      "metadata": {
        "id": "zmKLXBJlxZbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Raw Data, Initial Preprocessing And Basic analysis"
      ],
      "metadata": {
        "id": "dSSRuegnv72H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing Raw data\n",
        "df=pd.read_excel('/content/drive/MyDrive/SCA Projects/Forecasting /Raw Data for Forecasting Project.xlsx')\n"
      ],
      "metadata": {
        "id": "iG3re7AdxUcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Initial preprocessing(12-12-2021) work-Function\n",
        "def data_preprocess1(df):\n",
        "  df = df.drop(df.columns.to_series()[\"Unnamed: 6\":\"Receipt Details\"], axis=1)\n",
        "  df.columns = df.iloc[0]\n",
        "  df = df[1:].reset_index(drop=bool)\n",
        "  del df['Total']\n",
        "  df = df.melt(id_vars=[\"Part No.\", \"Unit Price\",'Category','Sales MOQ','Opening Stock'],\n",
        "               var_name=\"Month\",\n",
        "               value_name=\"demand\")\n",
        "  df['Month']=pd.to_datetime(df['Month']).dt.to_period('M')\n",
        "  df['Unit Price']=df['Unit Price'].astype('float')\n",
        "  df['Sales MOQ']=df['Sales MOQ'].astype('float')\n",
        "  df['Opening Stock']=df['Opening Stock'].astype('float')\n",
        "  df['demand']=df['demand'].astype('float')\n",
        "  return df"
      ],
      "metadata": {
        "id": "QkSIvWd4v63z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Initial preprocessing raw data\n",
        "df1 = data_preprocess1(df)"
      ],
      "metadata": {
        "id": "xxb2UDFJwGCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Basic Analysis Function for initial preprocessed data\n",
        "def basic_analysis(df):\n",
        "  print('*****Initial Information about Data*****\\n')\n",
        "  print(df.head())\n",
        "  print('\\nNumber of Rows and Columns:',df.shape,'\\n--------------------')\n",
        "  Null = df.isnull().sum()\n",
        "  Null = Null[Null>0]\n",
        "  print('missing data:\\n',Null)\n",
        "  print('--------------------\\nData Types:\\n',df.dtypes)\n",
        "  print('--------------------\\nCategorical variables:',df.select_dtypes(include=['object']).columns.to_list(),'And Number of columns:',len(df.select_dtypes(include=['object']).columns.to_list()))\n",
        "  print('--------------------\\nNumerical variables:',df.select_dtypes(include=[int,float]).columns.to_list(),'And Number of columns:',len(df.select_dtypes(include=[int,float]).columns.to_list()))\n",
        "  print('--------------------\\nDate time variables:',df.select_dtypes(include=['period[M]']).columns.to_list())\n",
        "  print('--------------------\\n')\n",
        "  print(round(df.describe()).astype(int))\n",
        "  print('--------------------\\n')\n",
        "  print(df.describe(include=object))\n",
        "  print('--------------------\\n')\n",
        "  print(df['Month'].describe())\n",
        "  print('--------------------\\n')\n"
      ],
      "metadata": {
        "id": "D_xtTE06wD48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic information about initial pre processed data\n",
        "basic_analysis(df1)"
      ],
      "metadata": {
        "id": "Ff5Y2sRWwRB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyphothesis Tests"
      ],
      "metadata": {
        "id": "bmJs4S9_2Bw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Hyphothesis Testing for distribution checking(normally distributed or not)\n",
        "alpha = 0.05\n",
        "H0 = 'Data is normal'\n",
        "Ha = 'Data is not normal'\n",
        "for i in df3.columns.to_list():\n",
        "  p = round(shapiro(df3[i])[1], 2)\n",
        "  print(f'**************{i}**************')\n",
        "  if p > alpha:\n",
        "    print(f\"{p} > {alpha}. We fail to reject Null Hypothesis. {H0}\")\n",
        "  else:\n",
        "    print(f\"{p} <= {alpha}. We reject Null Hypothesis. {Ha}\")\n",
        "  print('--------------\\n')\n"
      ],
      "metadata": {
        "id": "Rv4H3C6H2EgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Stationary test\n",
        "def adfuller_test(series, signif=0.05, name='', verbose=False):\n",
        "    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n",
        "    r = adfuller(series, autolag='AIC')\n",
        "    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n",
        "    p_value = output['pvalue'] \n",
        "    def adjust(val, length= 6): return str(val).ljust(length)\n",
        "\n",
        "    # Print Summary\n",
        "    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n",
        "    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n",
        "    print(f' Significance Level    = {signif}')\n",
        "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
        "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
        "\n",
        "    for key,val in r[4].items():\n",
        "        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n",
        "\n",
        "    if p_value <= signif:\n",
        "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
        "        print(f\" => Series is Stationary.\")\n",
        "    else:\n",
        "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
        "        print(f\" => Series is Non-Stationary.\") \n",
        "\n",
        "# ADF Test on each column\n",
        "for name, column in df3.iteritems():\n",
        "    adfuller_test(column, name=column.name)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "7E2ddr8J24YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##grangercausalitytests test\n",
        "maxlag=12\n",
        "test = 'ssr_chi2test'\n",
        "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
        "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
        "    The rows are the response variable, columns are predictors. The values in the table \n",
        "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
        "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
        "    zero, that is, the X does not cause Y can be rejected.\n",
        "\n",
        "    data      : pandas dataframe containing the time series variables\n",
        "    variables : list containing names of the time series variables.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
        "    for c in df.columns:\n",
        "        for r in df.index:\n",
        "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
        "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
        "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
        "            min_p_value = np.min(p_values)\n",
        "            df.loc[r, c] = min_p_value\n",
        "    df.columns = [var + '_x' for var in variables]\n",
        "    df.index = [var + '_y' for var in variables]\n",
        "    return df\n",
        "\n",
        "grangers_causation_matrix(df3, variables = df3.columns)  "
      ],
      "metadata": {
        "id": "oz-p-unl2kUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##coint_johansen test\n",
        "def cointegration_test(df, alpha=0.05): \n",
        "    \"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\n",
        "    out = coint_johansen(df,-1,5)\n",
        "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
        "    traces = out.lr1\n",
        "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
        "    def adjust(val, length= 6): return str(val).ljust(length)\n",
        "\n",
        "    # Summary\n",
        "    print('Name   ::  Test Stat > C(95%)    =>   Signif  \\n', '--'*20)\n",
        "    for col, trace, cvt in zip(df.columns, traces, cvts):\n",
        "        print(adjust(col), ':: ', adjust(round(trace,2), 9), \">\", adjust(cvt, 8), ' =>  ' , trace > cvt)\n",
        "\n",
        "cointegration_test(df3)"
      ],
      "metadata": {
        "id": "MPehpE-E3FVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SKU wise Forecasting -- VAR(Vector Auto Regression)\n"
      ],
      "metadata": {
        "id": "EXVb-dyLwWCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparing for SKU wise Forecasting\n",
        "df1 = df.groupby(by=['Month','Part No.'])['demand'].sum().reset_index()\n",
        "\n",
        "##converting categories into columns\n",
        "df2 = df1.pivot(index='Month', columns='Part No.', values='demand')"
      ],
      "metadata": {
        "id": "RSwxqooLwXm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting Data Into Train And Test\n",
        "train = df2[:int(0.8*(len(df2)))]\n",
        "valid = df2[int(0.8*(len(df2))):]\n"
      ],
      "metadata": {
        "id": "YK1oz0UEwjJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VAR Training\n",
        "model = VAR(endog=train)\n",
        "model_fit = model.fit()"
      ],
      "metadata": {
        "id": "iK2Xi0gGw1Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make prediction on validation\n",
        "prediction = model_fit.forecast(model_fit.y, steps=len(valid))"
      ],
      "metadata": {
        "id": "_e3K0TuqwqO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predictions into data frame\n",
        "df_pred = pd.DataFrame(prediction, columns = df2.columns.to_list())"
      ],
      "metadata": {
        "id": "k9_kYQUQw4Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error = pd.DataFrame({})\n",
        "for i in valid.columns.to_list():\n",
        "  er = round(mean_absolute_percentage_error(valid[i], df_pred[i]),3)\n",
        "  rmse = round(np.sqrt(mean_squared_error(valid[i], df_pred[i], squared=False)),3)\n",
        "  error=error.append({'Part No.':i,\n",
        "                                'mape':er,\n",
        "                                'rmse':rmse},ignore_index=True)\n",
        "error['mape_int'] = round(error['mape']).astype(int)"
      ],
      "metadata": {
        "id": "gyDsf0wCw6TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error.sort_values(by='mape_int',ascending=False)\n"
      ],
      "metadata": {
        "id": "3kj7UHUkxC9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## categorize wise Sales forecasting --VAR\n"
      ],
      "metadata": {
        "id": "-QW3dxiTxIbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting Data into sales wise from dimand wise\n",
        "df['sales'] = df['demand']*df['Unit Price']\n",
        "\n",
        "df1 = df.groupby(by=['Month','Category'])['sales'].sum().reset_index()"
      ],
      "metadata": {
        "id": "-ElHvyopxJRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparing for forecasting\n",
        "\n",
        "##converting categories into columns\n",
        "df2 = df1.pivot(index='Month', columns='Category', values='sales')\n",
        "#resetting default index\n",
        "df2 = df2.rename_axis(None).rename_axis(None, axis=1)"
      ],
      "metadata": {
        "id": "jj_W7LoPxoTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the train and validation set\n",
        "train = df2[:int(0.8*(len(df2)))]\n",
        "valid = df2[int(0.8*(len(df2))):]"
      ],
      "metadata": {
        "id": "_qGm_Stjxu0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "\n",
        "model = VAR(endog=train)\n",
        "model_fit = model.fit()"
      ],
      "metadata": {
        "id": "SyzL42X1xw2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make prediction on validation\n",
        "model_fit.summary()"
      ],
      "metadata": {
        "id": "AdqcC8xdxy2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make prediction on validation\n",
        "prediction = model_fit.forecast(model_fit.y, steps=len(valid))"
      ],
      "metadata": {
        "id": "kodbXXjmx2Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred = pd.DataFrame(prediction, columns = df2.columns.to_list())\n",
        "\n",
        "error = pd.DataFrame({})\n",
        "for i in valid.columns.to_list():\n",
        "  er = round(mean_absolute_percentage_error(valid[i], df_pred[i]),3)##mape calculation\n",
        "  rmse = round(np.sqrt(mean_squared_error(valid[i], df_pred[i], squared=False)),3)##rmse calculation\n",
        "  error=error.append({'Part No.':i,\n",
        "                                'mape':er,\n",
        "                                'rmse':rmse},ignore_index=True)\n",
        "\n",
        "error"
      ],
      "metadata": {
        "id": "gX1S4sNsx3yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## forecasting future\n",
        "months_to_forecast=20\n",
        "forcast_future = model_fit.forecast(model_fit.y, steps=months_to_forecast)\n",
        "forcast_future1 = pd.DataFrame(prediction, columns = df2.columns.to_list())\n",
        "forcast_future1.plot()"
      ],
      "metadata": {
        "id": "rDc5xwbnyEJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time series to Machine learning\n"
      ],
      "metadata": {
        "id": "gKTnugLuyIA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preparing_Time_series_to_supervise_ML(df):\n",
        "  #creating sales column\n",
        "  df['sales'] = df['demand']*df['Unit Price']\n",
        "  df1 = df.groupby(by=['Month','Category'])['sales'].sum().reset_index()\n",
        "  \n",
        "  ##converting categories into columns\n",
        "  df2 = df1.pivot(index='Month', columns='Category', values='sales')\n",
        "  #resetting default index\n",
        "  df2 = df2.rename_axis(None).rename_axis(None, axis=1)\n",
        "  col = df2.columns.to_list()\n",
        "  df3 = df2.copy()\n",
        "  df3 = round(df3).astype(int)\n",
        "  #t-1lag\n",
        "  \n",
        "  df3['Kit(t-1)'] = df3['Kit'].shift(1)\n",
        "  df3['Kit-Child Part(t-1)'] = df3['Kit-Child Part'].shift(1)\n",
        "  df3['Obsolete(t-1)'] = df3['Obsolete'].shift(1)\n",
        "  df3['Regular(t-1)'] = df3['Regular'].shift(1)\n",
        "  #t-2 lagdf3['Kit(t-2)'] = df3['Kit'].shift(2)\n",
        "  df3['Kit-Child Part(t-2)'] = df3['Kit-Child Part'].shift(2)\n",
        "  df3['Obsolete(t-2)'] = df3['Obsolete'].shift(2)\n",
        "  df3['Regular(t-2)'] = df3['Regular'].shift(2)\n",
        "  #t-3 lag\n",
        "  df3['Kit(t-3)'] = df3['Kit'].shift(3)\n",
        "  df3['Kit-Child Part(t-3)'] = df3['Kit-Child Part'].shift(3)\n",
        "  df3['Obsolete(t-3)'] = df3['Obsolete'].shift(3)\n",
        "  df3['Regular(t-3)'] = df3['Regular'].shift(3)\n",
        "  \n",
        "  df3.dropna(inplace=True)\n",
        "  \n",
        "  df3 = df3.reset_index()\n",
        "  df3['index']=pd.to_datetime(df3['index'])\n",
        "  df3['month'] = df3['index'].dt.month\n",
        "  df3['year'] = df3['index'].dt.year\n",
        "  \n",
        "  del df3['index']\n",
        "  return df3, col"
      ],
      "metadata": {
        "id": "ZUEmcPD8yIuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ML, col = data_preparing_Time_series_to_supervise_ML(df)\n"
      ],
      "metadata": {
        "id": "Rh4A0OsCyQlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seperating Dependent and Independent variables\n",
        "X=df_ML.copy()\n",
        "Y=df_ML[['Kit','Kit-Child Part','Obsolete','Regular']]\n",
        "del X['Kit']\n",
        "del X['Kit-Child Part']\n",
        "del X['Obsolete']\n",
        "del X['Regular']"
      ],
      "metadata": {
        "id": "3500pnHEy18n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## categorie Wise Sales forecasting -- With Linear Regression\n"
      ],
      "metadata": {
        "id": "vN4dIh36yWbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_perfamance(X,Y[i]):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,Y[i], test_size=0.20, random_state=42)\n",
        "  model = LinearRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "  y_train_predict = model.predict(X_train)\n",
        "  y_test_predict = model.predict(X_test)\n",
        "  print(f'MAPE Error of {i}:=--',mean_absolute_percentage_error(y_test,y_test_predict))\n",
        "  return y_test_predict,y_test\n",
        "\n",
        "\n",
        "error = pd.DataFrame({})\n",
        "print('********************Forecasting error with Linear regression********************')\n",
        "for i in col:\n",
        "  y_test_predict,y_test = model_perfamance(df_ML,Y[i])\n",
        "  er = mean_absolute_percentage_error(y_test,y_test_predict)##mape calculation\n",
        "  rmse = round(np.sqrt(mean_squared_error(y_test,y_test_predict, squared=False)),3)##rmse calculation\n",
        "  error=error.append({'Part No.':i,\n",
        "                                'mape':er,\n",
        "                                'rmse':rmse},ignore_index=True)\n",
        "\n",
        "round(error_mape,3)"
      ],
      "metadata": {
        "id": "123zfiyjyXBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## categorie Wise Sales forecasting -- With Decission Tree\n"
      ],
      "metadata": {
        "id": "OH8-zmJ30TIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_perfamance(X,Y[i]):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,Y[i], test_size=0.20, random_state=42)\n",
        "  model = DecisionTreeRegressor(max_depth=2)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_train_predict = model.predict(X_train)\n",
        "  y_test_predict = model.predict(X_test)\n",
        "  print(f'MAPE Error of {i}:=--',mean_absolute_percentage_error(y_test,y_test_predict))\n",
        "  return y_test_predict,y_test\n",
        "\n",
        "\n",
        "error = pd.DataFrame({})\n",
        "print('********************Forecasting error with DecisionTreeRegressor********************')\n",
        "for i in col:\n",
        "  y_test_predict,y_test = model_perfamance(df_ML,Y[i])\n",
        "  er = mean_absolute_percentage_error(y_test,y_test_predict)##mape calculation\n",
        "  rmse = round(np.sqrt(mean_squared_error(y_test,y_test_predict, squared=False)),3)##rmse calculation\n",
        "  error=error.append({'Part No.':i,\n",
        "                                'mape':er,\n",
        "                                'rmse':rmse},ignore_index=True)\n",
        "\n",
        "round(error_mape,3)"
      ],
      "metadata": {
        "id": "2gPuVsN60Qj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## categorie Wise Sales forecasting -- With Random Forest\n"
      ],
      "metadata": {
        "id": "sLmJGigF0kEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_perfamance(X,Y[i]):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,Y[i], test_size=0.20, random_state=42)\n",
        "  model = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_train_predict = model.predict(X_train)\n",
        "  y_test_predict = model.predict(X_test)\n",
        "  print(f'MAPE Error of {i}:=--',mean_absolute_percentage_error(y_test,y_test_predict))\n",
        "  return y_test_predict,y_test\n",
        "\n",
        "\n",
        "error = pd.DataFrame({})\n",
        "print('********************Forecasting error with RandomForestRegressor********************')\n",
        "for i in col:\n",
        "  y_test_predict,y_test = model_perfamance(df_ML,Y[i])\n",
        "  er = mean_absolute_percentage_error(y_test,y_test_predict)##mape calculation\n",
        "  rmse = round(np.sqrt(mean_squared_error(y_test,y_test_predict, squared=False)),3)##rmse calculation\n",
        "  error=error.append({'Part No.':i,\n",
        "                                'mape':er,\n",
        "                                'rmse':rmse},ignore_index=True)\n",
        "\n",
        "round(error_mape,3)"
      ],
      "metadata": {
        "id": "OK1LCRG60ko8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## categorie Wise Sales forecasting -- With XG Boost\n"
      ],
      "metadata": {
        "id": "0Alal5Im0vZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_perfamance(X,Y[i]):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,Y[i], test_size=0.20, random_state=42)\n",
        "  model =  XGBRegressor(n_estimators=10, max_depth=20, verbosity=2)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_train_predict = model.predict(X_train)\n",
        "  y_test_predict = model.predict(X_test)\n",
        "  print(f'MAPE Error of {i}:=--',mean_absolute_percentage_error(y_test,y_test_predict))\n",
        "  return y_test_predict,y_test\n",
        "\n",
        "\n",
        "error = pd.DataFrame({})\n",
        "print('********************Forecasting error with XGBRegressor********************')\n",
        "for i in col:\n",
        "  y_test_predict,y_test = model_perfamance(df_ML,Y[i])\n",
        "  er = mean_absolute_percentage_error(y_test,y_test_predict)##mape calculation\n",
        "  rmse = round(np.sqrt(mean_squared_error(y_test,y_test_predict, squared=False)),3)##rmse calculation\n",
        "  error=error.append({'Part No.':i,\n",
        "                                'mape':er,\n",
        "                                'rmse':rmse},ignore_index=True)\n",
        "\n",
        "round(error_mape,3)"
      ],
      "metadata": {
        "id": "ckSrjlXZ0wHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## categorie Wise Sales forecasting -- With Ada Boost\n"
      ],
      "metadata": {
        "id": "Y4EF4mo41TqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_perfamance(X,Y[i]):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,Y[i], test_size=0.20, random_state=42)\n",
        "  model =  AdaBoostRegressor(random_state=0, n_estimators=100\n",
        "  model.fit(X_train, y_train)\n",
        "  y_train_predict = model.predict(X_train)\n",
        "  y_test_predict = model.predict(X_test)\n",
        "  print(f'MAPE Error of {i}:=--',mean_absolute_percentage_error(y_test,y_test_predict))\n",
        "  return y_test_predict,y_test\n",
        "\n",
        "\n",
        "error = pd.DataFrame({})\n",
        "print('********************Forecasting error with AdaBoostRegressor********************')\n",
        "for i in col:\n",
        "  y_test_predict,y_test = model_perfamance(df_ML,Y[i])\n",
        "  er = mean_absolute_percentage_error(y_test,y_test_predict)##mape calculation\n",
        "  rmse = round(np.sqrt(mean_squared_error(y_test,y_test_predict, squared=False)),3)##rmse calculation\n",
        "  error=error.append({'Part No.':i,\n",
        "                                'mape':er,\n",
        "                                'rmse':rmse},ignore_index=True)\n",
        "\n",
        "round(error_mape,3)"
      ],
      "metadata": {
        "id": "V00N8hqg1UhS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}